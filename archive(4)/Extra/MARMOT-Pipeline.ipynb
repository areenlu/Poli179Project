{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f99eb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'MARMOT' already exists and is not an empty directory.\n",
      "/home/jandolina/teams/jack_areen_rubin/MARMOT\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: numpy in /home/jandolina/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting pathlib (from -r requirements.txt (line 3))\n",
      "  Using cached pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.4.2)\n",
      "Collecting sklearn (from -r requirements.txt (line 5))\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hrunning develop\n",
      "error: can't create or remove files in install directory\n",
      "\n",
      "The following error occurred while trying to add or remove files in the\n",
      "installation directory:\n",
      "\n",
      "    [Errno 13] Permission denied: '/opt/conda/lib/python3.9/site-packages/test-easy-install-290.write-test'\n",
      "\n",
      "The installation directory you specified (via --install-dir, --prefix, or\n",
      "the distutils default setting) was:\n",
      "\n",
      "    /opt/conda/lib/python3.9/site-packages/\n",
      "\n",
      "Perhaps your account does not have write access to this directory?  If the\n",
      "installation directory is a system-owned directory, you may need to sign in\n",
      "as the administrator or \"root\" account.  If you do not have administrative\n",
      "access to this machine, you may wish to choose a different installation\n",
      "directory, preferably one that is listed in your PYTHONPATH environment\n",
      "variable.\n",
      "\n",
      "For information on other options, you may wish to consult the\n",
      "documentation at:\n",
      "\n",
      "  https://setuptools.readthedocs.io/en/latest/easy_install.html\n",
      "\n",
      "Please make the appropriate changes for your system and try again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/patrickywu/MARMOT\n",
    "\n",
    "# Move into the cloned directory\n",
    "%cd MARMOT\n",
    "\n",
    "# Install the required Python packages\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Run the setup script to install the package in development mode\n",
    "!python setup.py develop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8c20c",
   "metadata": {},
   "source": [
    "# Init of bert, image, marmot models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52835b2a",
   "metadata": {},
   "source": [
    "to init bert:\n",
    "(bert pipeline)\n",
    "bert_model\n",
    "\n",
    "to init marmot:\n",
    "bert_model, image_model\n",
    "\n",
    "to init binary trainer:\n",
    "marmot_model, train_dataset, validation_dataset, epochs, learning_rate, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7032c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.modules.bert_pipeline import bert_textcaption_pipeline\n",
    "\n",
    "# Initialize a BERT model\n",
    "bert_model = 'bert-base-uncased'\n",
    "\n",
    "# Create an instance of bert_textcaption_pipeline\n",
    "textcaption_pipeline = bert_textcaption_pipeline(bert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63555058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50\n",
    "from marmot.modules.image_transformer import image_transformer\n",
    "\n",
    "# Instantiate a pre-trained ResNet-50 model\n",
    "image_model = resnet50(pretrained=True)\n",
    "\n",
    "# Create an instance of the image_transformer class\n",
    "image_translator = image_transformer(bert_model, image_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d4bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from marmot.modules.marmot_pipeline import marmot\n",
    "\n",
    "#  Instantiate the marmot class from the marmot module\n",
    "marmot = marmot(bert_model=bert_model, image_model=image_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8fd5f",
   "metadata": {},
   "source": [
    "# Create sample data for pipeline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382f2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import text\n",
    "data = pd.read_csv('/home/jandolina/teams/jack_areen_rubin/Data/fbpac-ads-en-US.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e77481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'html', 'political', 'not_political', 'title', 'message',\n",
      "       'thumbnail', 'created_at', 'updated_at', 'lang', 'images',\n",
      "       'impressions', 'political_probability', 'targeting', 'suppressed',\n",
      "       'targets', 'advertiser', 'entities', 'page', 'lower_page', 'targetings',\n",
      "       'paid_for_by', 'targetedness', 'listbuilding_fundraising_proba'],\n",
      "      dtype='object')\n",
      "{https://pp-facebook-ads.s3.amazonaws.com/v/t45.1600-4/cp0/q90/spS444/p180x540/49331592_23843377427170360_6166817401984778240_n.png.jpg}\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n",
    "print(data.images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc655bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23843377427170360_6166817401984778240_n.png\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "urls = [\n",
    "    \"https://pp-facebook-ads.s3.amazonaws.com/v/t45.1600-4/cp0/q90/spS444/p180x540/49331592_23843377427170360_6166817401984778240_n.png.jpg\",\n",
    "    # Add more URLs here if needed\n",
    "]\n",
    "\n",
    "# Regular expression pattern to extract the desired substring\n",
    "pattern = r'_([\\w-]+\\.png)'\n",
    "\n",
    "# Extract the desired substring for each URL\n",
    "for url in urls:\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59833adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         55844308_6127288747297_3018113389080608768_n.p...\n",
      "1         55863114_23843220221260433_8886009863356809216...\n",
      "2                                          wPe6V5iq-og.png}\n",
      "3         49331592_23843377427170360_6166817401984778240...\n",
      "4                                          u7XTTygYX3C.png}\n",
      "                                ...                        \n",
      "162319                                     -PAXP-deijE.gif}\n",
      "162320                                     -PAXP-deijE.gif}\n",
      "162321    39894768_23842963973330612_6596145833291284480...\n",
      "162322    55273822_23843395989240769_6786027694657110016...\n",
      "162323    37622419_23842883857610242_2096369976649711616...\n",
      "Name: filename, Length: 162324, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define a function to extract the filename from the urls\n",
    "pattern = r'/([^/]+)$'\n",
    "def extract_filename(url):\n",
    "    match = re.search(pattern, url)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        return(extracted_text)\n",
    "    else:\n",
    "        return('ERROR')\n",
    "\n",
    "# Apply the function to the column\n",
    "data['filename'] = data['images'].apply(extract_filename)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(data.filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6658c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#import images\n",
    "folder_path = '/home/jandolina/teams/jack_areen_rubin/Data/Images'\n",
    "\n",
    "filename_list = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "        filename_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88973133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(filename_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1eee88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270/474004382.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matching_rows['matched_local_path'] = ((folder_path + '/')+ (matching_rows['filename'].apply(get_matching_value)))\n"
     ]
    }
   ],
   "source": [
    "# Function to identify matches and extract the matching value\n",
    "def get_matching_value(filename):\n",
    "    for val in filename_list:\n",
    "        if val in filename:\n",
    "            return val\n",
    "    return None\n",
    "\n",
    "# Identify rows where 'filename' column matches values from the list\n",
    "matching_rows = data[data['filename'].apply(lambda x: any(val in x for val in filename_list))]\n",
    "\n",
    "# Create a new column 'matching_value' and assign the matching value to it\n",
    "matching_rows['matched_local_path'] = ((folder_path + '/')+ (matching_rows['filename'].apply(get_matching_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8f42e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'html', 'political', 'not_political', 'title', 'message',\n",
      "       'thumbnail', 'created_at', 'updated_at', 'lang', 'images',\n",
      "       'impressions', 'political_probability', 'targeting', 'suppressed',\n",
      "       'targets', 'advertiser', 'entities', 'page', 'lower_page', 'targetings',\n",
      "       'paid_for_by', 'targetedness', 'listbuilding_fundraising_proba',\n",
      "       'filename', 'matched_local_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(matching_rows.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49568eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            id  \\\n",
      "0                                6106619708591   \n",
      "1                                6106619718791   \n",
      "2                            23842636904200346   \n",
      "3                                6083430034527   \n",
      "4                                6081591643823   \n",
      "..                                         ...   \n",
      "71  hyperfeed_story_id_5c92c060156622346947409   \n",
      "72                           23842695844070311   \n",
      "73  hyperfeed_story_id_5cb229e0174828116868542   \n",
      "74                           23842645528590609   \n",
      "75                           23842656358710646   \n",
      "\n",
      "                                                 html  political  \\\n",
      "0   <div class=\"_5pcr userContentWrapper\"><div cla...          3   \n",
      "1   <div class=\"_5pcr userContentWrapper\"><div cla...          5   \n",
      "2   <div class=\"_1dwg _1w_m\"><div class=\"_4r_y\"><d...          2   \n",
      "3   <div class=\"_1dwg _1w_m\"><div class=\"_4r_y\"><d...          0   \n",
      "4   <div class=\"_1dwg _1w_m _q7o\"><div class=\"_4r_...          2   \n",
      "..                                                ...        ...   \n",
      "71  <div class=\"_5pa- userContentWrapper\"><div cla...          1   \n",
      "72  <div class=\"_1dwg _1w_m _q7o\"><div class=\"_5g-...          4   \n",
      "73  <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
      "74  <div class=\"_1dwg _1w_m _q7o\"><div class=\"_4r_...          0   \n",
      "75  <div class=\"_1dwg _1w_m\"><div class=\"_5g-l\"><d...          3   \n",
      "\n",
      "    not_political                             title  \\\n",
      "0               1                       Sierra Club   \n",
      "1               0                       Sierra Club   \n",
      "2               0        Dean Phillips for Congress   \n",
      "3               0      UNHCR, the UN Refugee Agency   \n",
      "4               0            Texas Democratic Party   \n",
      "..            ...                               ...   \n",
      "71              0                               USO   \n",
      "72              0                      Jason Kander   \n",
      "73              1                               USO   \n",
      "74              0                 Justice Democrats   \n",
      "75              0  Maine Center for Economic Policy   \n",
      "\n",
      "                                              message  \\\n",
      "0   <p>With the recent extreme weather events -- h...   \n",
      "1   <p>With the recent extreme weather events -- h...   \n",
      "2   <p>If elected in 2018, I’d be 1 of only 7 memb...   \n",
      "3   <p>More than 5 million Syrian refugees forced ...   \n",
      "4   <p>Texas Republicans' “Show me your papers” la...   \n",
      "..                                                ...   \n",
      "71  <p>When winter snowstorms are raging in Afghan...   \n",
      "72  <div class=\"mbs _5pbx\" id=\"js_7i\">You may know...   \n",
      "73  <p>When winter snowstorms are raging in Afghan...   \n",
      "74  <p>DNC Chair Tom Perez just purged high-rankin...   \n",
      "75  <p>Yes on 4: A Bipartisan Solution to Protect ...   \n",
      "\n",
      "                                            thumbnail  \\\n",
      "0   https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "1   https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "2   https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "3   https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "4   https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "..                                                ...   \n",
      "71  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "72  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "73  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "74  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "75  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "\n",
      "                       created_at                     updated_at   lang  ...  \\\n",
      "0   2018-09-22 00:04:13.443177+00  2018-09-27 04:10:10.394425+00  en-US  ...   \n",
      "1   2018-09-23 16:58:59.116609+00  2018-09-29 12:58:19.444037+00  en-US  ...   \n",
      "2   2017-10-28 02:48:55.870448+00  2017-11-02 12:57:58.113741+00  en-US  ...   \n",
      "3   2017-11-02 18:52:01.808253+00  2017-11-02 18:52:01.808253+00  en-US  ...   \n",
      "4   2017-11-15 22:49:01.449137+00  2017-12-27 18:12:23.518374+00  en-US  ...   \n",
      "..                            ...                            ...    ...  ...   \n",
      "71  2019-03-20 22:37:41.722713+00  2019-03-21 00:18:56.327721+00  en-US  ...   \n",
      "72  2018-03-07 19:43:01.360025+00    2018-03-09 15:41:34.6903+00  en-US  ...   \n",
      "73  2019-04-13 18:27:31.862184+00  2019-04-13 21:02:55.499792+00  en-US  ...   \n",
      "74  2017-11-29 22:13:08.732674+00  2017-11-29 22:13:08.732674+00  en-US  ...   \n",
      "75  2017-10-25 22:42:06.591487+00  2017-10-25 23:14:36.046864+00  en-US  ...   \n",
      "\n",
      "                                           lower_page  \\\n",
      "0                https://www.facebook.com/sierraclub/   \n",
      "1                https://www.facebook.com/sierraclub/   \n",
      "2   https://www.facebook.com/deanphillipsforcongress/   \n",
      "3                     https://www.facebook.com/unhcr/   \n",
      "4      https://www.facebook.com/texasdemocraticparty/   \n",
      "..                                                ...   \n",
      "71                   https://www.facebook.com/theuso/   \n",
      "72              https://www.facebook.com/jasonkander/   \n",
      "73                   https://www.facebook.com/theuso/   \n",
      "74         https://www.facebook.com/justicedemocrats/   \n",
      "75                   https://www.facebook.com/mecep1/   \n",
      "\n",
      "                                           targetings  paid_for_by  \\\n",
      "0   {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...  Sierra Club   \n",
      "1   {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...  Sierra Club   \n",
      "2   {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...          NaN   \n",
      "3   {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...          NaN   \n",
      "4   {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...          NaN   \n",
      "..                                                ...          ...   \n",
      "71                                                NaN      the USO   \n",
      "72  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...          NaN   \n",
      "73                                                NaN      the USO   \n",
      "74  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...          NaN   \n",
      "75                                                NaN          NaN   \n",
      "\n",
      "   targetedness listbuilding_fundraising_proba  \\\n",
      "0           6.0                       0.149693   \n",
      "1           3.0                       0.166995   \n",
      "2           7.0                       0.476530   \n",
      "3           4.0                       1.000038   \n",
      "4           7.0                       0.305509   \n",
      "..          ...                            ...   \n",
      "71          NaN                       0.264006   \n",
      "72          5.0                       0.397937   \n",
      "73          NaN                       0.264006   \n",
      "74          1.0                       0.492939   \n",
      "75          NaN                       0.696452   \n",
      "\n",
      "                                             filename  \\\n",
      "0   22536128_6083197966791_1310667523015835648_n.p...   \n",
      "1   22536128_6083197966791_1310667523015835648_n.p...   \n",
      "2   22536322_23842636886590346_1976273774701445120...   \n",
      "3   22536169_6082868194527_1915480166188974080_n.png}   \n",
      "4   22536042_6080423825223_5471680363921145856_n.png}   \n",
      "..                                                ...   \n",
      "71  22536023_6082903165107_3665698932949778432_n.p...   \n",
      "72  22536030_23842660378650311_5400078934064758784...   \n",
      "73  22536023_6082903165107_3665698932949778432_n.p...   \n",
      "74  22536043_23842636127730609_8525738673604395008...   \n",
      "75  22519572_10155289989544751_6320226996623100578...   \n",
      "\n",
      "                                   matched_local_path dummy_label pic caption  \n",
      "0   /home/jandolina/teams/jack_areen_rubin/Data/Im...           1   1          \n",
      "1   /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "2   /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "3   /home/jandolina/teams/jack_areen_rubin/Data/Im...           1   1          \n",
      "4   /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "..                                                ...         ...  ..     ...  \n",
      "71  /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "72  /home/jandolina/teams/jack_areen_rubin/Data/Im...           1   1          \n",
      "73  /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "74  /home/jandolina/teams/jack_areen_rubin/Data/Im...           1   1          \n",
      "75  /home/jandolina/teams/jack_areen_rubin/Data/Im...           0   1          \n",
      "\n",
      "[76 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "print(matching_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b7b8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_rows.to_csv('test_rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de60c1d",
   "metadata": {},
   "source": [
    "# Init binary data processing class\n",
    "\n",
    "Notes: why is caption_varname a parameter? whats the difference from text_varname? do I have to generate the captions first? Some errors shown in ID, badly collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc292a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270/2835860082.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matching_rows['dummy_label'] = np.random.randint(2, size=len(matching_rows))\n",
      "/home/jandolina/teams/jack_areen_rubin/MARMOT/marmot/data/binary_data_processing.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['pic'] = 1\n"
     ]
    }
   ],
   "source": [
    "from marmot.data.binary_data_processing import TextImageDatasetBinary\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "matching_rows.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# dummy label data\n",
    "matching_rows['dummy_label'] = np.random.randint(2, size=len(matching_rows))\n",
    "\n",
    "\n",
    "text_image_dataset = TextImageDatasetBinary(data = matching_rows , docid_varname = 'id', text_varname = 'message', \n",
    "                                            img_varname = 'matched_local_path', caption_varname = 'message', \n",
    "                                            label_varname = 'dummy_label', transform = transforms.Compose([\n",
    "    transforms.Resize((8.33, 11.11)),  # Resize the image to a fixed size\n",
    "    transforms.ToTensor(),            # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the image\n",
    "                                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa57a6f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270/2299818217.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_image_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/teams/jack_areen_rubin/MARMOT/marmot/data/binary_data_processing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mimage_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#print(type(image_raw))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m\"\"\"Read in Image Caption\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \"\"\"\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    474\u001b[0m             )\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected type {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "print(text_image_dataset.__getitem__(10)['image_caption'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c8e74",
   "metadata": {},
   "source": [
    "# Init text processor\n",
    "I think the image_captions need to be cleaned up. Right now these are just the text in the post, not image captions generated from the image. \n",
    "What does ii_text, tti_text, am_text represent?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb392af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.data.text_processor import text_processor\n",
    "import torch\n",
    "\n",
    "text_processor = text_processor(bert_model, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81d0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii_text, tti_text, am_text = text_processor.extract_bert_inputs(text_image_dataset.__getitem__(10)['image_caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a66f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ii_text)\n",
    "print(tti_text)\n",
    "print(am_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca49159",
   "metadata": {},
   "source": [
    "# BERT pipeline\n",
    "for now, just text. \n",
    "\n",
    "How do we interpret the output of forward? What does forward represent as a method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88973e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.modules.bert_pipeline import bert_text_only_pipeline\n",
    "\n",
    "# Initialize a BERT model\n",
    "bert_model = 'bert-base-uncased'\n",
    "\n",
    "# Create an instance of bert_textcaption_pipeline\n",
    "text_only_pipeline = bert_text_only_pipeline(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_pipeline.forward(ii_text, tti_text, am_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322f7447",
   "metadata": {},
   "source": [
    "# Binary Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.Adapting_Classes.PandasDataset import PandasDataset\n",
    "\n",
    "adpt_train_dataset = PandasDataset(matching_rows)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matching_rows.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aa9731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_trainer_text_inp(model, train_dataset, validation_dataset, epochs, learning_rate, batch_size,\n",
    "                        gradient_clipping=False, gc_value=1.0, proportion_warmup_steps=0.1, weight=None, device='cuda'):\n",
    "    '''\n",
    "    model: the MARMOT model input\n",
    "    train_dataset: the training data\n",
    "    validation_dataset: the validation data\n",
    "    epochs: number of epochs\n",
    "    learning_rate: the learning rate used for the AdamW optimizer\n",
    "    batch_size: the batch size\n",
    "    gradient_clipping: whether to activate gradient clipping or not (by default, clips at 1.0)\n",
    "    gc_value: gradient clipping value\n",
    "    proportion_warmup_steps: how many steps to warm up in the optimizer\n",
    "    weight: if any weight on the two classes\n",
    "    device: whether to run on CPU or GPU\n",
    "    '''\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = learning_rate,\n",
    "                      betas = (0.9,0.98),\n",
    "                      eps = 1e-8)\n",
    "    \n",
    "    #adpt_train_dataset = PandasDataset_inp(train_dataset)\n",
    "    \n",
    "    #adpt_validation_dataset = PandasDataset_inp(train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler = RandomSampler(train_dataset),\n",
    "                                  batch_size = batch_size)\n",
    "\n",
    "    validation_dataloader = DataLoader(validation_dataset)\n",
    "\n",
    "    total_steps = epochs*len(train_dataloader)\n",
    "    \n",
    "    print(f\"Total steps: {total_steps}, Type of total_steps: {type(total_steps)}\")\n",
    "    \n",
    "    num_warmup_steps = int(proportion_warmup_steps * total_steps)\n",
    "    \n",
    "    print(f\"Num warmup steps: {num_warmup_steps}, Type of num_warmup_steps: {type(num_warmup_steps)}\")\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps, num_cycles=0.5)\n",
    "\n",
    "    tp = text_processor(bert_model=model.bert_model, device=device)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}\")\n",
    "        print(enumerate(train_dataloader))\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print('step')\n",
    "            print(step)\n",
    "            print('batch')\n",
    "            print(batch)\n",
    "            if step % 40 == 0 and step != 0:\n",
    "                print(\"Batch {} of {}. Elapsed: {:.4f} seconds\".format(step, len(train_dataloader), time.time() - start_time))\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            text_ii, text_tti, text_am = tp.extract_bert_inputs(batch['text'])\n",
    "\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            out, _ = model(text_ii=text_ii, text_tti=text_tti, text_am=text_am)\n",
    "            loss = F.cross_entropy(input=out.view(-1,2), target=label, weight=weight, reduction='mean')\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            if gradient_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gc_value) # prevent exploding gradients issue\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        avg_training_loss = train_loss / len(train_dataloader)\n",
    "        print('(Epoch {} / {}): loss: {:.4f}'.format(epoch+1, epochs, avg_training_loss))\n",
    "        print('Time for Epoch: {:.4f}'.format(time.time() - start_time))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        y_predicted = []\n",
    "        y_proba = []\n",
    "        y_true = []\n",
    "\n",
    "        for val in validation_dataloader:\n",
    "            text_ii, text_tti, text_am = tp.extract_bert_inputs(val['text'])\n",
    "\n",
    "            val_label = val['label'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_out, _ = model(text_ii, text_tti, text_am)\n",
    "                val_out = val_out.view(-1,2)\n",
    "                val_proba = F.softmax(val_out, dim=1)\n",
    "                val_predicted = torch.argmax(val_out, dim=1)\n",
    "                val_loss_ind = F.cross_entropy(input=val_out, target=val_label, weight=weight, reduction='mean').item()\n",
    "            # Update y_predicted and y_true\n",
    "            y_predicted.append(val_predicted)\n",
    "            y_proba.append(val_proba[:,1])\n",
    "            y_true.append(val_label)\n",
    "\n",
    "            # Validation loss\n",
    "            val_loss += val_loss_ind\n",
    "\n",
    "        y_predicted = torch.cat(y_predicted)\n",
    "        y_proba = torch.cat(y_proba)\n",
    "        y_true = torch.cat(y_true)\n",
    "\n",
    "        # Metrics\n",
    "        val_accuracy = accuracy_score(y_true.to('cpu'), y_predicted.to('cpu'))\n",
    "        print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
    "\n",
    "        val_f1_weighted = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average='weighted')\n",
    "        print(\"Validation F1 Score (Weighted): {:.4f}\".format(val_f1_weighted))\n",
    "\n",
    "        val_f1_macro = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average='macro')\n",
    "        print(\"Validation F1 Score (Macro): {:.4f}\".format(val_f1_macro))\n",
    "\n",
    "        val_f1 = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Validation F1 Scores (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_f1))\n",
    "\n",
    "        val_precision = precision_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Precision (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_precision))\n",
    "\n",
    "        val_recall = recall_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Recall (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_recall))\n",
    "\n",
    "        val_rocauc = roc_auc_score(y_true.to('cpu'), y_proba.to('cpu'))\n",
    "        print(\"ROC AUC: {:.4f}\".format(val_rocauc))\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_dataloader)\n",
    "        print(\"Validation Loss: {:.4f}\".format(avg_val_loss))\n",
    "\n",
    "    return val_accuracy, val_f1_weighted, val_f1_macro, val_f1, val_precision, val_recall, val_rocauc, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6d4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.data.text_processor import text_processor\n",
    "from marmot.Adapting_Classes.PandasDataset import PandasDataset\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, confusion_matrix, classification_report, precision_score, recall_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e2960ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marmot.modules.image_transformer import image_transformer\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "class marmot(nn.Module):\n",
    "    def __init__(self, bert_model, image_model, pretrained_image_channels=2048, pretrained_image_dim=7, bert_dim=768,\n",
    "                dropout_p_final_clf=0.1, intermediate_layer_final_clf=768*4, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Record the bert_model\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "        # Image Translator\n",
    "        self.ImageTranslator = image_transformer(self.bert_model, image_model, pretrained_image_channels=2048, pretrained_image_dim=7, bert_dim=bert_dim)\n",
    "\n",
    "        # Encoder\n",
    "        self.bert_clf = BertModel.from_pretrained(self.bert_model, output_attentions=True)\n",
    "        triple_tti = nn.Embedding(3, bert_dim)\n",
    "        triple_tti.weight.data[:2].copy_(self.bert_clf.embeddings.token_type_embeddings.weight)\n",
    "        triple_tti.weight.data[2].copy_(self.bert_clf.embeddings.token_type_embeddings.weight.data.mean(dim=0) +\n",
    "                                        torch.randn(self.bert_clf.embeddings.token_type_embeddings.weight.data.mean(dim=0).size())*0.01)\n",
    "        self.bert_clf.embeddings.token_type_embeddings = triple_tti\n",
    "\n",
    "        # Classifier\n",
    "        self.final_clf = nn.Sequential(nn.Linear(bert_dim, intermediate_layer_final_clf),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(p=dropout_p_final_clf),\n",
    "                                       nn.Linear(intermediate_layer_final_clf, num_classes))\n",
    "\n",
    "    def forward(self, img, pic, caption_ii, caption_tti, caption_am, text_ii, text_tti, text_am):\n",
    "\n",
    "        # Translate image\n",
    "        img_translated = self.ImageTranslator(img=img, caption_ii=caption_ii, caption_tti=caption_tti, caption_am=caption_am*pic.unsqueeze(-1))\n",
    "\n",
    "        # Create position embeddings\n",
    "        pos_text = torch.arange(text_ii.shape[1], dtype=torch.long).to(img.device)\n",
    "        pos_text = pos_text.unsqueeze(0).expand(text_ii.shape[0], text_ii.shape[1])\n",
    "        pos_caption = torch.arange(caption_ii.shape[1], dtype=torch.long).to(img.device)\n",
    "        pos_caption = pos_caption.unsqueeze(0).expand(caption_ii.shape[0], caption_ii.shape[1])\n",
    "\n",
    "        # Create token type IDs to input\n",
    "        tti_caption = caption_tti + 1\n",
    "        img_translated_tti = caption_tti + 2\n",
    "        tti = torch.cat((text_tti, tti_caption, img_translated_tti), dim=1)\n",
    "\n",
    "        # Create attention masks to input\n",
    "        am = torch.cat((text_am, caption_am*pic.unsqueeze(-1), caption_am*pic.unsqueeze(-1)), dim=1)\n",
    "\n",
    "        # Obtain inputs embeds\n",
    "        text_embeds = self.bert_clf.embeddings.word_embeddings(text_ii)\n",
    "        caption_embeds = self.bert_clf.embeddings.word_embeddings(caption_ii)\n",
    "        img_translated_fts = img_translated.last_hidden_state + self.bert_clf.embeddings(input_ids=caption_ii, token_type_ids=caption_tti, position_ids=pos_caption)\n",
    "\n",
    "        inputs_embeds = torch.cat((text_embeds, caption_embeds, img_translated_fts), dim=1)\n",
    "\n",
    "        # Create position ids for the image\n",
    "        pos_translated_img = torch.arange(img_translated.last_hidden_state.shape[1], dtype=torch.long).to(img.device)\n",
    "        pos_translated_img = pos_translated_img.unsqueeze(0).expand(img_translated_fts.shape[0], img_translated_fts.shape[1])\n",
    "\n",
    "        pos = torch.cat((pos_text, pos_caption, pos_translated_img), dim=1)\n",
    "\n",
    "        # BERT output\n",
    "        bert_output = self.bert_clf(attention_mask=am, token_type_ids=tti, position_ids=pos, inputs_embeds=inputs_embeds)\n",
    "\n",
    "        # Classifier\n",
    "        bert_output_avg = torch.sum(bert_output[0]*am.unsqueeze(-1), dim=1) / (torch.sum(am, dim=1).unsqueeze(-1))\n",
    "        out = self.final_clf(bert_output_avg)\n",
    "\n",
    "        return out, bert_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce33de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PandasDataset_inp(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataframe.iloc[idx]\n",
    "        text = item['message']\n",
    "        label = int(item['dummy_label'])  # Convert label to integer if it's stored as a string\n",
    "        return {'text': text, 'label': label}\n",
    "\n",
    "    \n",
    "train_dataset =PandasDataset_inp(matching_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1988fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270/415391859.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matching_rows['caption'] = None\n",
      "/tmp/ipykernel_270/415391859.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  matching_rows['caption'].fillna('', inplace=True)\n",
      "/tmp/ipykernel_270/415391859.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  matching_rows['caption'].fillna('', inplace=True)\n",
      "/home/jandolina/teams/jack_areen_rubin/MARMOT/marmot/data/binary_data_processing.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data['pic'] = 1\n"
     ]
    }
   ],
   "source": [
    "from marmot.data.binary_data_processing import TextImageDatasetBinary\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "matching_rows['caption'] = None\n",
    "\n",
    "# Replace None values in the 'caption' column with empty strings\n",
    "matching_rows['caption'].fillna('', inplace=True)\n",
    "\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: Image.fromarray(x)),\n",
    "    transforms.Resize((40, 25)),               # Resize the image to a fixed size\n",
    "    transforms.ToTensor(),                       # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],   # Normalize the image\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = TextImageDatasetBinary(data = matching_rows , docid_varname = 'id', text_varname = 'message', \n",
    "                                            img_varname = 'matched_local_path', caption_varname = 'caption', \n",
    "                                            label_varname = 'dummy_label', transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0568338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad766ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_trainer_inp(model, train_dataset, validation_dataset, epochs, learning_rate, batch_size,\n",
    "                   epoch_freeze_img=0, epoch_freeze_txt=0, gradient_clipping=False, gc_value=1.0, proportion_warmup_steps=0.1, weight=None, device='cpu'):\n",
    "    '''\n",
    "    model: the MARMOT model input\n",
    "    train_dataset: the training data\n",
    "    validation_dataset: the validation data\n",
    "    epochs: number of epochs\n",
    "    learning_rate: the learning rate used for the AdamW optimizer\n",
    "    batch_size: the batch size\n",
    "    epoch_freeze_img: the number of epochs to freeze the image translator\n",
    "    epoch_freeze_txt: the number of epochs to freeze the BERT encoder\n",
    "    gradient_clipping: whether to activate gradient clipping or not (by default, clips at 1.0)\n",
    "    gc_value: gradient clipping value\n",
    "    proportion_warmup_steps: how many steps to warm up in the optimizer\n",
    "    weight: if any weight on the two classes\n",
    "    device: whether to run on CPU or GPU\n",
    "    '''\n",
    "\n",
    "    optimizer = AdamW(model.parameters(model),\n",
    "                      lr = learning_rate,\n",
    "                      betas = (0.9,0.98),\n",
    "                      eps = 1e-8)\n",
    "    \n",
    "    #adpt_train_dataset = PandasDataset_inp(train_dataset)\n",
    "    \n",
    "    #adpt_validation_dataset = PandasDataset_inp(train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler = RandomSampler(train_dataset),\n",
    "                                  batch_size = batch_size)\n",
    "\n",
    "    validation_dataloader = DataLoader(validation_dataset)\n",
    "\n",
    "    total_steps = epochs*len(train_dataloader)\n",
    "\n",
    "    def get_linear_then_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_freeze_steps, num_cycles=0.5):\n",
    "        def lr_lambda(current_step: int):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1.0, num_warmup_steps))\n",
    "            elif current_step < num_freeze_steps:\n",
    "                return 1.0\n",
    "            else:\n",
    "                progress = float(current_step - num_warmup_steps - num_freeze_steps) / float(max(1, num_training_steps - num_warmup_steps - num_freeze_steps))\n",
    "                return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "        return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    scheduler = get_linear_then_cosine_schedule_with_warmup(optimizer=optimizer, num_warmup_steps=proportion_warmup_steps*total_steps, num_training_steps=total_steps, num_freeze_steps=epoch_freeze_txt*len(train_dataloader))\n",
    "    tp = text_processor(bert_model=model.bert_model, device=device)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "\n",
    "        if epoch_freeze_img > 0 or epoch_freeze_txt > 0:\n",
    "            if epoch < epoch_freeze_img:\n",
    "                for param in model.ImageTranslator.ImageDecoder.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in model.ImageTranslator.ImageDecoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "            if epoch < epoch_freeze_txt:\n",
    "                for param in model.bert_clf.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in model.final_clf.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for param in model.bert_clf.parameters():\n",
    "                    param.requires_grad = True\n",
    "                for param in model.final_clf.parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            print(step)\n",
    "            print(batch)\n",
    "            if step % 40 == 0 and step != 0:\n",
    "                print(\"Batch {} of {}. Elapsed: {:.4f} seconds\".format(step, len(train_dataloader), time.time() - start_time))\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            caption_ii, caption_tti, caption_am = tp.extract_bert_inputs(batch['image_caption'])\n",
    "            text_ii, text_tti, text_am = tp.extract_bert_inputs(batch['text'])\n",
    "\n",
    "            img = batch['image'].to(device)\n",
    "            \n",
    "            pic = batch['pic'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            \n",
    "            text_ii= text_ii.to(device)\n",
    "            text_tti = text_tti.to(device)\n",
    "            text_am = text_am.to(device)\n",
    "            caption_ii = caption_ii.to(device)\n",
    "            caption_tti = caption_tti.to(device)\n",
    "            caption_am = caption_am.to(device)\n",
    "\n",
    "            out, _ = model(img=img, pic=pic, caption_ii=caption_ii, caption_tti=caption_tti, caption_am=caption_am, text_ii=text_ii, text_tti=text_tti, text_am=text_am)\n",
    "            loss = F.cross_entropy(input=out.view(-1,2), target=label, weight=weight, reduction='mean')\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            if gradient_clipping:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gc_value) # prevent exploding gradients issue\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        avg_training_loss = train_loss / len(train_dataloader)\n",
    "        print('(Epoch {} / {}): loss: {:.4f}'.format(epoch+1, epochs, avg_training_loss))\n",
    "        print('Time for Epoch: {:.4f}'.format(time.time() - start_time))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        y_predicted = []\n",
    "        y_proba = []\n",
    "        y_true = []\n",
    "\n",
    "        for val in validation_dataloader:\n",
    "            caption_ii, caption_tti, caption_am = tp.extract_bert_inputs(val['image_caption'])\n",
    "            text_ii, text_tti, text_am = tp.extract_bert_inputs(val['text'])\n",
    "\n",
    "            img = val['image'].to(device)\n",
    "            pic = val['pic'].to(device)\n",
    "            val_label = val['label'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_out, _ = model(img, pic, caption_ii, caption_tti, caption_am, text_ii, text_tti, text_am)\n",
    "                val_out = val_out.view(-1,2)\n",
    "                val_proba = F.softmax(val_out, dim=1)\n",
    "                val_predicted = torch.argmax(val_out, dim=1)\n",
    "                val_loss_ind = F.cross_entropy(input=val_out, target=val_label, weight=weight, reduction='mean').item()\n",
    "            # Update y_predicted and y_true\n",
    "            y_predicted.append(val_predicted)\n",
    "            y_proba.append(val_proba[:,1])\n",
    "            y_true.append(val_label)\n",
    "\n",
    "            # Validation loss\n",
    "            val_loss += val_loss_ind\n",
    "\n",
    "        y_predicted = torch.cat(y_predicted)\n",
    "        y_proba = torch.cat(y_proba)\n",
    "        y_true = torch.cat(y_true)\n",
    "\n",
    "        # Metrics\n",
    "        val_accuracy = accuracy_score(y_true.to('cpu'), y_predicted.to('cpu'))\n",
    "        print(\"Validation Accuracy: {:.4f}\".format(val_accuracy))\n",
    "\n",
    "        val_f1_weighted = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average='weighted')\n",
    "        print(\"Validation F1 Score (Weighted): {:.4f}\".format(val_f1_weighted))\n",
    "\n",
    "        val_f1_macro = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average='macro')\n",
    "        print(\"Validation F1 Score (Macro): {:.4f}\".format(val_f1_macro))\n",
    "\n",
    "        val_f1 = f1_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Validation F1 Scores (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_f1))\n",
    "\n",
    "        val_precision = precision_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Precision (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_precision))\n",
    "\n",
    "        val_recall = recall_score(y_true.to('cpu'), y_predicted.to('cpu'), average=None)\n",
    "        print(\"Recall (By Class): Not Hit: {:.4f}, Hit: {:.4f}\".format(*val_recall))\n",
    "\n",
    "        val_rocauc = roc_auc_score(y_true.to('cpu'), y_proba.to('cpu'))\n",
    "        print(\"ROC AUC: {:.4f}\".format(val_rocauc))\n",
    "\n",
    "        avg_val_loss = val_loss / len(validation_dataloader)\n",
    "        print(\"Validation Loss: {:.4f}\".format(avg_val_loss))\n",
    "\n",
    "    return val_accuracy, val_f1_weighted, val_f1_macro, val_f1, val_precision, val_recall, val_rocauc, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c20ac33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "named_parameters() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_270/1161943831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmarmot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_trainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinary_trainer_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbinary_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_trainer_inp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarmot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_270/3248937352.py\u001b[0m in \u001b[0;36mbinary_trainer_inp\u001b[0;34m(model, train_dataset, validation_dataset, epochs, learning_rate, batch_size, epoch_freeze_img, epoch_freeze_txt, gradient_clipping, gc_value, proportion_warmup_steps, weight, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     '''\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     optimizer = AdamW(model.parameters(model),\n\u001b[0m\u001b[1;32m     20\u001b[0m                       \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                       \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.98\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, correct_bias, no_deprecation_warning)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid epsilon value: {eps} - should be >= 0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mdefaults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"lr\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weight_decay\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"correct_bias\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcorrect_bias\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \"\"\"\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: named_parameters() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "from marmot.scripts.binary_trainer import binary_trainer\n",
    "from marmot.scripts.binary_trainer import binary_trainer_text\n",
    "\n",
    "binary_trainer = binary_trainer_inp(marmot, train_dataset, train_dataset, epochs=1, learning_rate=1e-5, batch_size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8243d3",
   "metadata": {},
   "source": [
    "# Binary classifer.\n",
    "\n",
    "Do I need to run forward() methods before the binary classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f88595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
