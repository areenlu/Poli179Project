{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b53cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#import text\n",
    "data = pd.read_csv('/home/jandolina/teams/jack_areen_rubin/Data/fbpac-ads-en-US.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0cb308",
   "metadata": {},
   "source": [
    "# Using presence of words to determine whether an ad is related to the 2nd amendment or gun policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58fa92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_related_words = ['gun','2nd amendment', 'second amendment', 'ar-15', 'assault rifle', 'pistols', 'shooting', 'mass shooting', 'school shooting', 'march for our lives']\n",
    "\n",
    "# Function to check if any word from the list is present in the text\n",
    "def contains_word(text, word_list):\n",
    "    for word in word_list:\n",
    "        if word in text:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Create the new column\n",
    "data['dummy_label'] = data['message'].apply(lambda x: contains_word(x, gun_related_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e67d",
   "metadata": {},
   "source": [
    "# Data Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f928c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 162324\n",
      "Average Document Length (in words): 55.08601315886745\n",
      "\n",
      "Distribution of Document Lengths:\n",
      "count    162324.000000\n",
      "mean         55.086013\n",
      "std         103.500739\n",
      "min           1.000000\n",
      "25%          22.000000\n",
      "50%          36.000000\n",
      "75%          57.000000\n",
      "max        2976.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Balance of the Dataset:\n",
      "dummy_label\n",
      "0    0.972555\n",
      "1    0.027445\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Number of Documents\n",
    "num_documents = len(data)\n",
    "\n",
    "# Average Document Length (in terms of words)\n",
    "data['word_count'] = data['message'].apply(lambda x: len(str(x).split()))\n",
    "average_doc_length = data['word_count'].mean()\n",
    "\n",
    "# Distribution of Document Lengths\n",
    "doc_length_distribution = data['word_count'].describe()\n",
    "\n",
    "# Balance of the Dataset\n",
    "label_distribution = data['dummy_label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Number of Documents:\", num_documents)\n",
    "print(\"Average Document Length (in words):\", average_doc_length)\n",
    "print(\"\\nDistribution of Document Lengths:\")\n",
    "print(doc_length_distribution)\n",
    "print(\"\\nBalance of the Dataset:\")\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46b4c85",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6319159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words associated with gun legislation:\n",
      "gun: 238.4225606855614\n",
      "violence: 140.2045461840269\n",
      "congress: 103.94849943441567\n",
      "help: 99.16536413251045\n",
      "petition: 96.11251731509047\n",
      "pp: 92.72110110178767\n",
      "guns: 92.22281608193832\n",
      "people: 89.22734646432878\n",
      "sign: 88.34178056687114\n",
      "make: 86.0165474003404\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "# Filter documents related to gun legislation\n",
    "gun_legislation_messages = [data['message'][i] for i, label in enumerate(data['dummy_label']) if label == 1]\n",
    "\n",
    "# Custom function for text cleaning\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Create TF-IDF vectorizer with text cleaning and stop words removal\n",
    "vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, preprocessor=clean_text)\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(gun_legislation_messages)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum TF-IDF scores for each word across documents\n",
    "word_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Sort the words by TF-IDF score in descending order\n",
    "sorted_word_indices = word_scores.argsort()[0, ::-1]\n",
    "\n",
    "# Display the top words associated with gun legislation\n",
    "print(\"Top words associated with gun legislation:\")\n",
    "for i in range(10):  # Display top 10 words\n",
    "    word_index = sorted_word_indices[0, i]\n",
    "    word = feature_names[word_index]\n",
    "    score = word_scores[0, word_index]\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44cd55",
   "metadata": {},
   "source": [
    "# LDA\n",
    "Need to remove the html elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad305186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for each topic:\n",
      "Topic 0:\n",
      "people, pp, gun, time, day, violence, class_afxspan, class_cl, _afzspanspan, world\n",
      "Topic 1:\n",
      "gun, help, violence, pp, congress, guns, petition, need, make, background\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer with text cleaning and stop words removal\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, preprocessor=clean_text)\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(gun_legislation_messages)\n",
    "\n",
    "# Perform LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Change n_components as needed\n",
    "lda.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(\"Top words for each topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words_indices = topic.argsort()[:-11:-1]  # Display top 10 words\n",
    "    top_words = [feature_names[i] for i in top_words_indices]\n",
    "    print(\", \".join(top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5970d6",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b3d1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/jandolina/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis results:\n",
      "                                                id  \\\n",
      "0       hyperfeed_story_id_5c9baa3ee0ec08073500042   \n",
      "1       hyperfeed_story_id_5c9bb2a2413852086735771   \n",
      "2       hyperfeed_story_id_5c9bb4fa461731e29426627   \n",
      "3                                23843380741530360   \n",
      "4       hyperfeed_story_id_5c9bb059454851c17741213   \n",
      "...                                            ...   \n",
      "162319                           23843108782710078   \n",
      "162320                           23843034525850259   \n",
      "162321                           23842997138670612   \n",
      "162322  hyperfeed_story_id_5c8b16b11b8f86515960964   \n",
      "162323                           23842885237930242   \n",
      "\n",
      "                                                     html  political  \\\n",
      "0       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
      "1       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
      "2       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
      "3       <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
      "4       <div class=\"_5pa- userContentWrapper\"><div cla...          0   \n",
      "...                                                   ...        ...   \n",
      "162319  <div class=\"_5pcr userContentWrapper\"><div cla...         12   \n",
      "162320  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
      "162321  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
      "162322  <div class=\"_5pa- userContentWrapper\"><div cla...          7   \n",
      "162323  <div class=\"_5pcr userContentWrapper\"><div cla...          0   \n",
      "\n",
      "        not_political                                        title  \\\n",
      "0                   0                League of Conservation Voters   \n",
      "1                   0                            Indivisible Guide   \n",
      "2                   0               International Rescue Committee   \n",
      "3                   0                 Covenant House International   \n",
      "4                   1                           Planned Parenthood   \n",
      "...               ...                                          ...   \n",
      "162319              0                        Keep Them Accountable   \n",
      "162320              0  National Republican Congressional Committee   \n",
      "162321              0                              POW Action Fund   \n",
      "162322              0                                Beto O'Rourke   \n",
      "162323              0                                         ACLU   \n",
      "\n",
      "                                                  message  \\\n",
      "0       <p>BREAKING: Trump’s Department of the Interio...   \n",
      "1       <p>The Mueller investigation is over. Special ...   \n",
      "2       <p>Zimbabwe is reeling from the impact of Cycl...   \n",
      "3       <p>What more can you do in the final hours of ...   \n",
      "4       <p>Say it loud, say it proud: Our rights, our ...   \n",
      "...                                                   ...   \n",
      "162319  <p>Rep. Katko voted for tax breaks for his wea...   \n",
      "162320  <p>Illinois early voting is open NOW &amp; you...   \n",
      "162321  <p>From your favorite peaks to the polling pla...   \n",
      "162322  <p>Beto just announced he’s running for presid...   \n",
      "162323  <p>Claim your FREE ACLU Voter sticker today to...   \n",
      "\n",
      "                                                thumbnail  \\\n",
      "0       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "1       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "2       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "3       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "4       https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "...                                                   ...   \n",
      "162319  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "162320  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "162321  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "162322  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "162323  https://pp-facebook-ads.s3.amazonaws.com/v/t1....   \n",
      "\n",
      "                           created_at                     updated_at   lang  \\\n",
      "0       2019-03-27 16:52:25.625455+00  2019-03-27 16:52:25.625455+00  en-US   \n",
      "1       2019-03-27 17:28:14.096849+00  2019-03-27 17:28:14.096849+00  en-US   \n",
      "2       2019-03-27 17:38:23.101377+00  2019-03-27 17:38:23.101377+00  en-US   \n",
      "3       2018-12-30 20:59:13.879124+00  2018-12-30 20:59:13.879124+00  en-US   \n",
      "4       2019-03-27 17:18:29.764002+00  2019-04-11 15:02:58.081112+00  en-US   \n",
      "...                               ...                            ...    ...   \n",
      "162319  2018-10-19 10:31:52.466563+00   2018-10-22 11:40:06.24382+00  en-US   \n",
      "162320  2018-10-24 20:41:42.111865+00  2018-10-24 20:41:42.111865+00  en-US   \n",
      "162321   2018-10-09 20:03:32.81012+00   2018-10-09 20:03:32.81012+00  en-US   \n",
      "162322  2019-03-15 03:07:40.590249+00   2019-03-22 17:01:05.36319+00  en-US   \n",
      "162323  2018-08-08 02:54:51.076959+00  2018-08-08 20:03:36.302904+00  en-US   \n",
      "\n",
      "        ...                                               page  \\\n",
      "0       ...                 https://www.facebook.com/LCVoters/   \n",
      "1       ...         https://www.facebook.com/indivisibleguide/   \n",
      "2       ...  https://www.facebook.com/InternationalRescueCo...   \n",
      "3       ...            https://www.facebook.com/CovenantHouse/   \n",
      "4       ...        https://www.facebook.com/PlannedParenthood/   \n",
      "...     ...                                                ...   \n",
      "162319  ...    https://www.facebook.com/KeepThemAccountable18/   \n",
      "162320  ...                     https://www.facebook.com/NRCC/   \n",
      "162321  ...            https://www.facebook.com/POWActionFund/   \n",
      "162322  ...              https://www.facebook.com/betoorourke/   \n",
      "162323  ...                     https://www.facebook.com/aclu/   \n",
      "\n",
      "                                               lower_page  \\\n",
      "0                      https://www.facebook.com/lcvoters/   \n",
      "1              https://www.facebook.com/indivisibleguide/   \n",
      "2       https://www.facebook.com/internationalrescueco...   \n",
      "3                 https://www.facebook.com/covenanthouse/   \n",
      "4             https://www.facebook.com/plannedparenthood/   \n",
      "...                                                   ...   \n",
      "162319    https://www.facebook.com/keepthemaccountable18/   \n",
      "162320                     https://www.facebook.com/nrcc/   \n",
      "162321            https://www.facebook.com/powactionfund/   \n",
      "162322              https://www.facebook.com/betoorourke/   \n",
      "162323                     https://www.facebook.com/aclu/   \n",
      "\n",
      "                                               targetings  \\\n",
      "0                                                     NaN   \n",
      "1                                                     NaN   \n",
      "2                                                     NaN   \n",
      "3       {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
      "4                                                     NaN   \n",
      "...                                                   ...   \n",
      "162319  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
      "162320  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
      "162321  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
      "162322                                                NaN   \n",
      "162323  {\"<div><div class=\\\"_4-i0 _26c5\\\"><div class=\\...   \n",
      "\n",
      "                                              paid_for_by targetedness  \\\n",
      "0                           League of Conservation Voters          NaN   \n",
      "1                                     Indivisible Project          NaN   \n",
      "2                          International Rescue Committee          NaN   \n",
      "3                            Covenant House International          5.0   \n",
      "4                Planned Parenthood Federation of America          NaN   \n",
      "...                                                   ...          ...   \n",
      "162319  HOUSE MAJORITY PAC, (202) 849-6052, AND PRIORI...          7.0   \n",
      "162320  the NRCC and not authorized by any candidate o...          4.0   \n",
      "162321                    Protect Our Winters Action Fund          4.0   \n",
      "162322                                   Beto for America          NaN   \n",
      "162323                                           the ACLU          6.0   \n",
      "\n",
      "       listbuilding_fundraising_proba contains_word word_count dummy_label  \\\n",
      "0                            0.647945             0         37           0   \n",
      "1                            0.350635             0         78           0   \n",
      "2                            0.999909             0         34           0   \n",
      "3                                 NaN             0         20           0   \n",
      "4                            0.999977             0         19           0   \n",
      "...                               ...           ...        ...         ...   \n",
      "162319                       0.116965             0         14           0   \n",
      "162320                       0.312412             0         20           0   \n",
      "162321                       0.205220             0         35           0   \n",
      "162322                       0.999994             0         45           0   \n",
      "162323                       0.354132             0         21           0   \n",
      "\n",
      "       sentiment  \n",
      "0       Negative  \n",
      "1       Positive  \n",
      "2       Negative  \n",
      "3       Positive  \n",
      "4       Positive  \n",
      "...          ...  \n",
      "162319  Positive  \n",
      "162320   Neutral  \n",
      "162321  Positive  \n",
      "162322   Neutral  \n",
      "162323  Positive  \n",
      "\n",
      "[162324 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "nltk.download('vader_lexicon')  # Download the lexicon required for sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis for each message\n",
    "sentiments = []\n",
    "for message in data['message']:\n",
    "    sentiment_score = sid.polarity_scores(message)\n",
    "    # Classify sentiment based on compound score\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "# Add the sentiment labels to the DataFrame\n",
    "data['sentiment'] = sentiments\n",
    "\n",
    "# Display the DataFrame with sentiment labels\n",
    "print(\"Sentiment analysis results:\")\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b213b4",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "591336d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1158/545778796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the English NER model in spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NER model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with spaCy NER\n",
    "doc = nlp(text)\n",
    "\n",
    "# Function to perform Named Entity Recognition (NER) using spaCy\n",
    "def perform_ner(text):\n",
    "    doc = nlp(text)\n",
    "    named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return named_entities\n",
    "\n",
    "# Apply NER to each text in the 'text' column\n",
    "data['named_entities'] = data['text'].apply(perform_ner)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
