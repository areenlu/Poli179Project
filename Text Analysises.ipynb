{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b53cc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 200\n",
      "Average Document Length (in words): 50.19\n",
      "\n",
      "Distribution of Document Lengths:\n",
      "count    200.00000\n",
      "mean      50.19000\n",
      "std       46.56852\n",
      "min        7.00000\n",
      "25%       23.00000\n",
      "50%       40.00000\n",
      "75%       60.50000\n",
      "max      370.00000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Balance of the Dataset:\n",
      "dummy_label\n",
      "0    0.975\n",
      "1    0.025\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Top words associated with gun legislation:\n",
      "fair: 0.489678716613991\n",
      "background: 0.45312097254862194\n",
      "congress: 0.4351420831574906\n",
      "safe: 0.4007581783546891\n",
      "nra: 0.3858366759919961\n",
      "districts: 0.3672590374604932\n",
      "action: 0.35761331334392155\n",
      "stop: 0.3527582118648766\n",
      "gun: 0.3527582118648766\n",
      "sign: 0.32453484921680203\n",
      "\n",
      "Top words for each topic:\n",
      "Topic 0:\n",
      "congress, background, check, brady, moving, safe, gun, stop, nra, sign\n",
      "Topic 1:\n",
      "fair, districts, ballot, effort, current, th, necessary, signatures, november, room\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Areen\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment analysis results:\n",
      "                                               message sentiment\n",
      "0    <p>Refugee women are breadwinners, caretakers ...  Positive\n",
      "1    <p>Is your church ready for Refugee Sunday?</p...  Positive\n",
      "2    <p>We're launching a new version of IN THESE T...  Positive\n",
      "3    <p>We’re a campaign that deeply believes that ...  Negative\n",
      "4    <p>These women are pee’d off.</p><p> Curious? ...  Positive\n",
      "..                                                 ...       ...\n",
      "195  <p>I am asking you to make a contribution to o...  Positive\n",
      "196  <span class=\"fwn fcg\"><span class=\"fcg\"><span ...  Positive\n",
      "197  <p>“One thing that’s obvious is that the defea...  Negative\n",
      "198  <p>I’ll be blunt: I need you to take my poll b...  Positive\n",
      "199  <p>The biggest network of conservatives you'll...  Positive\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define the list of gun-related words\n",
    "gun_related_words = ['gun', '2nd amendment', 'second amendment', 'ar-15', 'assault rifle', 'pistols', 'shooting', 'mass shooting', 'school shooting', 'march for our lives']\n",
    "\n",
    "# Function to check if any word from the list is present in the text\n",
    "def contains_word(text, word_list):\n",
    "    if pd.isnull(text):  # Check if the text is NaN\n",
    "        return 0\n",
    "    for word in word_list:\n",
    "        if word.lower() in text.lower():  # Case insensitive match\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "file_path = 'areen-prehandcode.csv' \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Create the new column 'dummy_label'\n",
    "data['dummy_label'] = data['message'].apply(lambda x: contains_word(x, gun_related_words))\n",
    "\n",
    "# Calculate number of documents\n",
    "num_documents = len(data)\n",
    "\n",
    "# Calculate average document length (in terms of words)\n",
    "data['word_count'] = data['message'].apply(lambda x: len(str(x).split()))\n",
    "average_doc_length = data['word_count'].mean()\n",
    "\n",
    "# Distribution of document lengths\n",
    "doc_length_distribution = data['word_count'].describe()\n",
    "\n",
    "# Balance of the dataset\n",
    "label_distribution = data['dummy_label'].value_counts(normalize=True)\n",
    "\n",
    "print(\"Number of Documents:\", num_documents)\n",
    "print(\"Average Document Length (in words):\", average_doc_length)\n",
    "print(\"\\nDistribution of Document Lengths:\")\n",
    "print(doc_length_distribution)\n",
    "print(\"\\nBalance of the Dataset:\")\n",
    "print(label_distribution)\n",
    "\n",
    "# Filter documents related to gun legislation\n",
    "gun_legislation_messages = [data['message'][i] for i, label in enumerate(data['dummy_label']) if label == 1]\n",
    "\n",
    "# Custom function for text cleaning\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Convert ENGLISH_STOP_WORDS to a list\n",
    "stop_words_list = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Create TF-IDF vectorizer with text cleaning and stop words removal\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_list, preprocessor=clean_text)\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(gun_legislation_messages)\n",
    "\n",
    "# Get feature names (words)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum TF-IDF scores for each word across documents\n",
    "word_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Sort the words by TF-IDF score in descending order\n",
    "sorted_word_indices = word_scores.argsort()[0, ::-1]\n",
    "\n",
    "# Display the top words associated with gun legislation\n",
    "print(\"\\nTop words associated with gun legislation:\")\n",
    "for i in range(10):  # Display top 10 words\n",
    "    word_index = sorted_word_indices[0, i]\n",
    "    word = tfidf_feature_names[word_index]\n",
    "    score = word_scores[0, word_index]\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# Create CountVectorizer with text cleaning and stop words removal\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words_list, preprocessor=clean_text)\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = count_vectorizer.fit_transform(gun_legislation_messages)\n",
    "\n",
    "# Perform LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)  # Change n_components as needed\n",
    "lda.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTop words for each topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    top_words_indices = topic.argsort()[:-11:-1]  # Display top 10 words\n",
    "    top_words = [feature_names[i] for i in top_words_indices]\n",
    "    print(\", \".join(top_words))\n",
    "\n",
    "# Perform sentiment analysis for each message\n",
    "nltk.download('vader_lexicon')  # Download the lexicon required for sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis for each message\n",
    "sentiments = []\n",
    "for message in data['message']:\n",
    "    sentiment_score = sid.polarity_scores(message)\n",
    "    # Classify sentiment based on compound score\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "# Add the sentiment labels to the DataFrame\n",
    "data['sentiment'] = sentiments\n",
    "\n",
    "# Display the DataFrame with sentiment labels\n",
    "print(\"\\nSentiment analysis results:\")\n",
    "print(data[['message', 'sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0565b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset containing political text data (e.g., speeches, news articles)\n",
    "# Replace 'political_data.csv' with the path to your dataset\n",
    "political_data = pd.read_csv('areen-prehandcode.csv')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "nltk.download('vader_lexicon')  # Download the lexicon required for sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis for each political text\n",
    "political_data['sentiment_score'] = political_data['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "\n",
    "# Classify sentiment based on the compound score\n",
    "political_data['sentiment'] = political_data['sentiment_score'].apply(lambda x: 'Positive' if x >= 0.05 else ('Negative' if x <= -0.05 else 'Neutral'))\n",
    "\n",
    "# Analyze sentiment distribution\n",
    "sentiment_distribution = political_data['sentiment'].value_counts(normalize=True)\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(sentiment_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
